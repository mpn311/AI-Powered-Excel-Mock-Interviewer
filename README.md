# AI-Powered Excel Mock Interviewer

# Deployed Link: https://interview.apkzube.com/

## ğŸš€ Project Overview
The **AI-Powered Excel Mock Interviewer** is a web-based application that simulates Excel technical interviews, evaluates candidate responses, and generates professional feedback reports.  
It helps organizations **automate and standardize Excel skill assessments**, reducing interviewer workload and improving fairness.

---

## ğŸŒŸ Key Features
- **Structured Interview Flow** â†’ Candidate introduction, multi-turn Q&A, closing summary.  
- **Intelligent Answer Evaluation** â†’ Google Gemini LLM (`gemini-1.5-flash-latest`) with rubric-based scoring.  
- **Agentic Behavior** â†’ Maintains context across questions, acts like a real interviewer.  
- **Constructive Feedback Report** â†’ Scores, rationale, and overall proficiency rating.  
- **Automated Logging** â†’ Saves transcripts & reports with timestamps in `/logs`.  
- **Professional UI** â†’ Flask + Bootstrap 5 with progress bar, accordions, and score cards.  

---

## ğŸ—ï¸ Technical Architecture
**Flow:**
1. Candidate enters profile (name, experience).  
2. Questions loaded dynamically from `data/excel_questions.json`.  
3. Candidate answers â†’ evaluated by Gemini LLM with expected answer + rubric.  
4. LLM outputs strict JSON `{score, rationale}`.  
5. Scores + rationale stored in session.  
6. Summary page â†’ interactive accordion + score card.  
7. Transcript & feedback report auto-saved in `/logs`.  

---
# Technology Stack (Detailed Justification)

## 1) LLM â€” Google Gemini (gemini-1.5-flash-latest)
**Why this?**  
- **Costâ€“performance sweet spot:** Flash is optimized for speed and lower cost vs. â€œProâ€ models, which keeps inference bills down while still handling instruction following, tool-use style prompts, and JSON outputs reliably.  
- **Great for evaluation workflows:** Consistent, fast responses are ideal for multi-turn interview flows (ask â†’ evaluate â†’ feedback) without user-visible lag.  
- **Structured output friendliness:** Works well with schema-guided prompts (e.g., â€œreturn valid JSON with fields: score, rubric, suggestionsâ€), which simplifies downstream parsing.  

## 2) Framework â€” LangChain (Python)
**Why this?**  
- **Prompt templates & chaining:** Cleanly separates your system/role prompts from variable inputs; easy to build multi-step flows (ask â†’ evaluate â†’ summarize â†’ store).  
- **State & memory:** Conversation memory (buffer/summary) avoids manual bookkeeping across turns.  
- **Structured output parsing:** `PydanticOutputParser` (or LC equivalent) to enforce JSON schemas, reducing brittle string parsing.  
- **Integrations:** Ready adapters for Gemini + easy swap if you A/B test other models later.  

## 3) Backend â€” Python Flask
**Why this?**  
- **Lightweight & familiar:** Minimal boilerplate; perfect for APIs that proxy LLM calls and handle uploads.  
- **Production-ready with simple stack:** Gunicorn + Nginx works well; easy to containerize.  

## 4) Frontend â€” Bootstrap 5
**Why this?**  
- **Fast to ship:** Grid, forms, buttons, modals out-of-the-box â†’ you focus on core logic.  
- **Consistent & accessible:** Good default a11y and responsive behavior for dashboards and forms.  

## 5) Storage â€” JSON Question Bank (`excel_questions.json`)
**Why this?**  
- **Human-editable & versionable:** Non-tech stakeholders can update questions/rubrics via PRs.  
- **Zero ops:** No DB admin required for early stages.  

## 6) Logging â€” `/logs` folder for transcripts & feedback
**Why this?**  
- **Traceability:** Every interview run can be replayed (inputs, model outputs, score, rubric).  
- **Offline analysis:** Simple local JSONL is easy to mine for QA and model prompt improvements.  

## 7) Hosting â€” AWS EC2
**Why this?**  
- **Full control:** You own the box; easy to tune system dependencies (imagemagick, ffmpeg, etc. if later needed).  
- **Predictable pricing:** Good for steady loads; simpler than managed PaaS when you already know Linux basics.  

---

## ğŸ“‚ Project Structure
```
excel-mock-interviewer/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ templates/        # HTML templates (intro, question, summary, base)
â”‚   â”œâ”€â”€ static/css/       # Stylesheets
â”‚   â”œâ”€â”€ evaluator.py      # LLM evaluation & logging
â”‚   â””â”€â”€ routes.py         # Flask routes & logic
â”‚â”€â”€ data/
â”‚   â””â”€â”€ excel_questions.json  # Question bank
â”‚â”€â”€ logs/
â”‚   â”œâ”€â”€ transcripts/      # Saved Q&A transcripts
â”‚   â””â”€â”€ reports/          # Saved feedback reports
â”‚â”€â”€ run.py                
â”‚â”€â”€ wsgi.py               
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

## â„ï¸ Cold Start Strategy
Since no dataset exists initially:
1. **Bootstrapping** â€“ Use curated `excel_questions.json` with expert-written expected answers & evaluation criteria.  
2. **Iterative Improvement** â€“ Log transcripts & reports â†’ review by Excel experts â†’ refine prompts.  
3. **Dataset Building** â€“ Collect anonymized transcripts for fine-tuning.  
4. **Future Optimization** â€“ Fine-tune smaller LLMs  

---

## ğŸ“‘ Deliverables
- âœ… Design Document & Approach Strategy (`.docx`)  
- âœ… Flask-based Proof-of-Concept app  
- âœ… Deployed version (AWS EC2)  
- âœ… Sample transcripts & feedback in `/logs`  

---

## ğŸš€ Future Enhancements
- Skill-wise analytics dashboard (charts).  
- Adaptive questioning (difficulty adjustment).  
 
---

ğŸ‘¨â€ğŸ’» **Author:** [Milan Nagvadiya]  
